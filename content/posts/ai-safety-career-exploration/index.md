---
title: "AI Safety Career Exploration"
draft: false
summary: "A newbie's attempt to strategize his way into AI safety technical research."
date: 2025-09-20T10:00:00Z
tags: ["career", "resources"]
categories: ["Career"]
weight: 1
cover:
    image: ""
    alt: ""
    caption: ""
    relative: false
    hidden: true
ShowToc: true
UseHugoToc: true
TocOpen: false
ShowReadingTime: true
ShowPostNavLinks: false
---
## How Is This Blog Different

The biggest difference between this blog and other career advice resources is that it represents a newcomer's real-time attempt to break into AI safety technical research. Here are a few reasons why you might find it worth reading:

- üßë‚Äçüíª **Working professionals friendly**: Most existing career advice is geared toward students or PhDs. One of my goals is to offer more realistic, practical guidance for working software engineers like myself who are already employed full-time.
- üîç **Focus on *how* to use resources**: Rather than just listing resources, I provide actual explanations about why and how to use them effectively. While others have done excellent work compiling comprehensive resource lists, I aim to offer insights on navigating these resources when you first encounter them.

**Note:** This blog is primarily written with technical alignment research in mind, since that's the path I'm pursuing.

### Career advice blogs

I personally found the following career advice blogs contain useful information:

[How To Become A Mechanistic Interpretability¬†Researcher](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher) by Neel Nanda (Sep 2025) ‚≠êÔ∏è

[So you want to work on technical AI¬†safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety) by George Wang (Jun 2024) ‚≠êÔ∏è

[AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/) by Benjamin Hilton (Jun 2023) ([LessWrong version](https://www.lesswrong.com/posts/oXkgGAgJ35yNDRvzF/new-career-review-ai-safety-technical-research))

[AGI safety career advice](https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice) by Richard Ngo (May 2023) ‚≠êÔ∏è

[Levelling Up in AI Safety Research¬†Engineering](https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering) by GabeM (Sep 2022)

[How to pursue a career in technical AI¬†alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment) by Charlie Rogers-Smith (Jun 2022)  ‚≠êÔ∏è

[FAQ: Advice for AI alignment researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/) by Rohin Shah (Jan 2021)

Below, I'll highlight several concrete principles for someone like me to get started.

## Mentorship

This is the first thing that *every* person and career advice blog I came across mentioned.

In Richard Ngo's [AGI safety career advice](https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice), he specifically emphasizes:

> If you have little research experience, your main priority should be finding the best mentor possible to help you gain research skills

Ethan Perez echoes this in [Tips for Empirical Alignment Research](https://www.alignmentforum.org/posts/dZFpEdKyb9Bf4xYn7/tips-for-empirical-alignment-research):

> Close mentorship is maybe the fastest path to become an expert in a domain, and there's a lot of evidence to support this.

The data backs this up: In MATS' alumni evaluation, **72%** said mentorship was either the most valuable aspect or represented a major share of the program's value.

Here's why I think mentorship is considered so critically important in technical alignment research:

- **Fast feedback loops** on your work and thinking
- **Network connections** through your mentor's professional relationships
- **Higher publication success** rates through guided research
- **Proven track record** - mentoring correlates with better career and productivity outcomes across many research domains, not just alignment

The central importance of mentorship leads to one of the key challenges in entering technical alignment research: **Alignment is mentorship-constrained**. The field remains small, with far fewer experienced researchers than aspiring newcomers.

Here are a few ways to secure mentorship without quitting your job:

1. **Part-time research programs** (e.g., SPAR, MARS, etc. - see the *research programs* section)
2. **Direct outreach** to professors or PhD students working on AI safety to see if you can contribute to their papers

## Have Something to Show

This is another universal recommendation across all the career advice I've encountered.

George Wang (research lead at [Timaeus](https://timaeus.co/)) explains the incremental approach in [So you want to work on technical AI safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety):

> The general idea here is to do a small thing to show that you're a good candidate for a medium thing, then do a medium thing to show you can do a bigger thing, and so on.

You need to give interviewers, mentors, or professors a concrete reason to work with you. A portfolio serves as a strong signal to **prevent false positives**‚Äîhiring someone they shouldn't have hired.


Neel Nanda emphasizes this point in [How To Become A Mechanistic Interpretability Researcher](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher):

> Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors, **public research output is your best credential**. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc.

What hiring managers ultimately look for:

> Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly.

## Skip Forward Aggressively

George Wang advocates for an accelerated approach in [So you want to work on technical AI safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety):

> As a general note, **I don't recommend that you try to do everything listed in depth**. I'm trying not to leave huge gaps here, but you can and should try to skip forward aggressively, and you'll probably find that you're ready for later steps much sooner than you think you are.

Let's say the entire learning roadmap looks like this:
![[Pasted image 20250911121314.png]]

I should attempt step n+1 or even n+2 while I'm still working on step n. Sometimes I might already be capable of performing step n+1 despite still being at step n. Other times, I'll predictably hit a wall or blocker when trying to reach ahead.

But hitting that wall is actually valuable‚Äînow I know exactly what to work on next. That's progress in itself. If I hadn't tried applying, I might still be over-studying unnecessary material or remain unaware of the actual requirements.

I believe the general principle is to leap forward quickly, especially given that AI safety timelines are short. I don't need to become a top researcher to land a research role‚ÄîI just need to meet the minimum bar and learn the rest on the job.

This approach is validated by real hiring practices. In a [blog post about hiring for safety researchers](https://www.lesswrong.com/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment), the Google DeepMind team stated:

> As a rough test for the Research Engineer role, if you can reproduce a typical ML paper in a few hundred hours and your interests align with ours, we're probably interested in interviewing you.


## Excel at Your Current Job

Excelling in your current role builds essential **career capital**‚Äîthe skills, reputation, professional network, and financial runway that top alignment teams actively seek when hiring.

Equally important: **strong software engineering skills are non-negotiable** for technical alignment positions. Frontier research labs predominantly hire "research engineers" who design experiments, build specialized tooling, and manage large distributed systems. These requirements align perfectly with the experience gained from high-performing software engineering roles.

**Example:** [Redwood Research careers page](https://www.redwoodresearch.org/careers)
![[Pasted image 20250913203036.png]]

## Resources

### Resource hubs

[Events &amp; training](https://www.aisafety.com/events-and-training) by AISafety.com  ‚≠êÔ∏è

[AGI careers](https://80000hours.org/agi/) by 80,000 Hours (they also offer [1-1 career advising](https://80000hours.org/speak-with-us/?int_campaign=primary-navigation))

[AI Safety Papers](https://arkose.org/aisafety) & [Opportunities](https://arkose.org/opportunities) by Arkose

### Organizations

[Map of AI safety orgs](https://www.aisafety.com/map) by AISafety.com  ‚≠êÔ∏è - the most comprehensive landscape for ai safety related orgs

### Research programs

[MATS](https://www.matsprogram.org/) (in-person, full-time, Berkeley/London)

[SPAR](https://sparai.org/) (part-time)

[MARS](https://www.cambridgeaisafety.org/mars) (part-time)

[Astra Fellowship](https://www.constellation.org/programs/astra-fellowship) by [Constellation](https://www.constellation.org/) (in-person, full-time, Berkeley)
