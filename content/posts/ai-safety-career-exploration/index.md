---
title: "AI Safety Career Exploration"
draft: false
summary: "A newbie's attempt to strategize his way into AI safety technical research."
date: 2025-09-20T10:00:00Z
tags: ["career", "resources"]
categories: ["Career"]
weight: 1
cover:
    image: ""
    alt: ""
    caption: ""
    relative: false
    hidden: true
ShowToc: true
UseHugoToc: true
TocOpen: false
ShowReadingTime: true
ShowPostNavLinks: false
---
## How Is This Blog Different

The biggest difference between this blog and other career advice resources is that it represents a newcomer's real-time attempt to break into AI safety technical research. Here are a few reasons why you might find it worth reading:

- üßë‚Äçüíª **Working professionals friendly**: Most existing career advice is geared toward students or PhDs. One of my goals is to offer more realistic, practical guidance for working software engineers like myself who are already employed full-time.
- üîç **Focus on *how* to use resources**: Rather than just listing resources, I provide actual explanations about why and how to use them effectively. While others have done excellent work compiling comprehensive resource lists, I aim to offer insights on navigating these resources when you first encounter them.

**Note:** This blog is primarily written with technical alignment research in mind, since that's the path I'm pursuing.

### Career advice blogs

I personally found the following career advice blogs contain useful information:

[How To Become A Mechanistic Interpretability¬†Researcher](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher) by Neel Nanda (Sep 2025) ‚≠êÔ∏è

[So you want to work on technical AI¬†safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety) by George Wang (Jun 2024) ‚≠êÔ∏è

[AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/) by Benjamin Hilton (Jun 2023) ([LessWrong version](https://www.lesswrong.com/posts/oXkgGAgJ35yNDRvzF/new-career-review-ai-safety-technical-research))

[AGI safety career advice](https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice) by Richard Ngo (May 2023) ‚≠êÔ∏è

[Levelling Up in AI Safety Research¬†Engineering](https://forum.effectivealtruism.org/posts/S7dhJR5TDwPb5jypG/levelling-up-in-ai-safety-research-engineering) by GabeM (Sep 2022)

[How to pursue a career in technical AI¬†alignment](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment) by Charlie Rogers-Smith (Jun 2022)  ‚≠êÔ∏è

[FAQ: Advice for AI alignment researchers](https://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/) by Rohin Shah (Jan 2021)

Below, I'll highlight several concrete principles for someone like me to get started.

## Mentorship

This is the first thing that *every* person and career advice blog I came across mentioned.

In Richard Ngo's [AGI safety career advice](https://www.lesswrong.com/posts/ho63vCb2MNFijinzY/agi-safety-career-advice), he specifically emphasizes:

> If you have little research experience, your main priority should be finding the best mentor possible to help you gain research skills

Ethan Perez echoes this in [Tips for Empirical Alignment Research](https://www.alignmentforum.org/posts/dZFpEdKyb9Bf4xYn7/tips-for-empirical-alignment-research):

> Close mentorship is maybe the fastest path to become an expert in a domain, and there's a lot of evidence to support this.

The data backs this up: In MATS' alumni evaluation, **72%** said mentorship was either the most valuable aspect or represented a major share of the program's value.

Here's why I think mentorship is considered so critically important in technical alignment research:

- **Fast feedback loops** on your work and thinking
- **Network connections** through your mentor's professional relationships
- **Higher publication success** rates through guided research
- **Proven track record** - mentoring correlates with better career and productivity outcomes across many research domains, not just alignment

The central importance of mentorship leads to one of the key challenges in entering technical alignment research: **Alignment is mentorship-constrained**. The field remains small, with far fewer experienced researchers than aspiring newcomers.

Here are a few ways to secure mentorship without quitting your job:

1. **Part-time research programs** (e.g., SPAR, MARS, etc. - see the *research programs* section)
2. **Direct outreach** to professors or PhD students working on AI safety to see if you can contribute to their papers

# have something to show

this is another thing that everyone recommends.

George Wang (research lead at [Timaeus](https://timaeus.co/)) in [So you want to work on technical AI¬†safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety):

> The general idea here is to do a small thing to show that you‚Äôre a good candidate for a medium thing, then do a medium thing to show you can do a bigger thing, and so on.

you need to give the interviewer/mentor/professor a reason to work with you. a portfolio is a strong signal to prevent false positives (hiring someone you **shouldn‚Äôt** have hired).
(take myself as an example, if i want to cold email a professor or a potential mentor, i don't even know how to, because i don't have anything to show them yet. i can't just be like "oh i'm super interested please give me a chance")

Neel Nanda in [How To Become A Mechanistic Interpretability¬†Researcher](https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher):

> Further, if you want to pursue a career in the space, whether a job, a PhD, or just informally working with mentors,¬†**public research output is your best credential**. It's very clear and concrete proof that you are competent, can execute on research and do interesting things, and this is exactly the kind of evidence people care about seeing if they're trying to figure out whether they should work with you, pay attention to what you're saying, etc.

what do hiring managers look for:

> Your goal as a candidate is to provide compelling, hard-to-fake evidence of your skills. The best way to do that is to simply do good research and share it publicly.

# skip forward aggressively

George Wang in [So you want to work on technical AI¬†safety](https://www.lesswrong.com/posts/kGt3ukLR924kyfn5y/so-you-want-to-work-on-technical-ai-safety):

> As a general note,¬†**I don't recommend that you try to do everything listed in depth**. I'm trying not to leave huge gaps here, but you can and should try to skip forward aggressively, and you'll probably find that you're ready for later steps much sooner than you think you are.

let's say the entire road looks like this:
![[Pasted image 20250911121314.png]]

i should try doing step n+1 or even n+ 2 while i'm still at still n. sometimes i might already able to perform step n+1 despite i'm still at n. though sometimes i'd predictably hit a wall or blocker when i'm trying to reach ahead. (happened to me as i'm trying to get into these programs without even knowing how neural networks work üòÇ) but it's okay if you hit a wall, now at least you know what exactly to work on, that's also progress! if i didn't try applying, i might still be over-studying unnecessary stuff or don't even know what's required.

but i think the general rule is to leap forward quickly. because timelines are short enough, and i don't need to be a top researcher to get a research role. i just need to meet the bar and learn the rest on the job.

In a¬†[blog post about hiring for safety researchers](https://www.lesswrong.com/posts/nzmCvRvPm4xJuqztv/deepmind-is-hiring-for-the-scalable-alignment-and-alignment), the Google DeepMind team said:

> As a rough test for the Research Engineer role, if you can reproduce a typical ML paper in a few hundred hours and your interests align with ours, we‚Äôre probably interested in interviewing you.


# excel at your current job

Excelling where you are builds **career capital**‚Äîskills, reputation, mentors, and runway‚Äîthat top alignment teams actually hire for.

Just as importantly, **strong software engineering is a must-have** for technical alignment roles: frontier labs overwhelmingly hire ‚Äúresearch engineers‚Äù who design experiments, build tooling, and operate large distributed systems‚Äîrequirements that map directly to high-performing SWE experience.

Example: [Redwood Research careers page](https://www.redwoodresearch.org/careers)
![[Pasted image 20250913203036.png]]

## Resources

### Resource hubs

[Events &amp; training](https://www.aisafety.com/events-and-training) by AISafety.com  ‚≠êÔ∏è

[AGI careers](https://80000hours.org/agi/) by 80,000 Hours (they also offer [1-1 career advising](https://80000hours.org/speak-with-us/?int_campaign=primary-navigation))

[AI Safety Papers](https://arkose.org/aisafety) & [Opportunities](https://arkose.org/opportunities) by Arkose

### Organizations

[Map of AI safety orgs](https://www.aisafety.com/map) by AISafety.com  ‚≠êÔ∏è - the most comprehensive landscape for ai safety related orgs

### Research programs

[MATS](https://www.matsprogram.org/) (in-person, full-time, Berkeley/London)

[SPAR](https://sparai.org/) (part-time)

[MARS](https://www.cambridgeaisafety.org/mars) (part-time)

[Astra Fellowship](https://www.constellation.org/programs/astra-fellowship) by [Constellation](https://www.constellation.org/) (in-person, full-time, Berkeley)
